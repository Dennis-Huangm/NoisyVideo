# QuickStart

Before initiating the evaluation, please configure the environment according to the specified procedures. 
Once configured, you may utilize `run.py` and `evaluation.py` to perform a comprehensive and multiple perspectives assessment of the target Video-LLM.

## Step 0: Setup API keys
To use API models (e.g., GPT-4v, Gemini-Pro-V) for inference, or to utilize the LLM API as an evaluator or selector-extractor, you must first configure your API key.

 - ​ If you need to use the API, enter your key in the corresponding key field. These API keys will be automatically loaded during inference and evaluation. You can place the required API keys in the `$VLMEvalKit/.env` file or set them directly as environment variables. If you choose to create a `.env` file, its contents should look like this:
 ```
# The .env file, place it under $VLMEvalKit
# API Keys of Proprietary VLMs
# QwenVL APIs
DASHSCOPE_API_KEY=
# Gemini w. Google Cloud Backends
GOOGLE_API_KEY=
# OpenAI API
OPENAI_API_KEY=
OPENAI_API_BASE=
# StepAI API
STEPAI_API_KEY=
# REKA API
REKA_API_KEY=
# GLMV API
GLMV_API_KEY=
# CongRong API
CW_API_BASE=
CW_API_KEY=
# SenseNova API
SENSENOVA_API_KEY=
# Hunyuan-Vision API
HUNYUAN_SECRET_KEY=
HUNYUAN_SECRET_ID=
# LMDeploy API
LMDEPLOY_API_BASE=
# You can also set a proxy for calling api models during the evaluation stage
EVAL_PROXY=
```

 ## Step 1: Installation
 First, create a conda virtual environment and activate it:
```
conda create --name ${your_env_name} python==3.9
cd VLMEvalKit
pip install -e .
pip install flash-attn --no-build-isolation
```
Some models cannot be directly invoked via the `transformers` library and require manual installation from their source code. For ​**​Chat-UniVi​**​, ​**​LLaMA-VID​**​, and ​**​LLaVA​**​, you must `cd their_source_code_directory` and run `pip install -e .` to install them. Note that **​LLaMA-VID-7B​** may not be able to run under certain transformer versions, we recommend to use `transformers==4.31.0` while evaluating.

# Step 2: Prediction

We use `run.py` to get the prediction under diverse visual noise and the basic **GPT score** judged by conventional LLMs.

Our toolkit supports the evaluation of **any Video-LLM**. Here, we demonstrate the evaluation process using **Qwen2.5-VL-3B-Instruct** and **Gaussian Noise** as an example.


**Argrments**

 - `--data`: Set the dataset names. In our benchmark, we test different Video-LLMs by applying noise to **MMBench-Video**.
 - `--model`: Set the Video-LLM names currently supported. 
 - `--judge`: Set the API model names as the **judge**. We adopt gpt-4o in our benchmark.
 - `--ratio`: Set the ratio of noisy frames in the input video.
 - `--noise_name`: Set the noise names you want to evluate, you can find all 36 supported noise types in the following code:
```python
  from video_noise.noise_applier import NoiseRegistry
  print(NoiseRegistry.list_noises())
```
**Command for evaluating a local model**
```shell
torchrun \
	--nproc-per-node=${NUM_GPUS}  \
	run.py \
	--data MMBench_Video_${NUM_FRAMES}frame_nopack \
	--model ${MODEL} \
	--judge gpt-4o \
	--noise_name ${NOISE_TYPE} \
	--ratio ${NOISE_PROPORTION}
```
Example:
```
torchrun --nproc-per-node=2 run.py --data MMBench_Video_8frame_nopack --model Qwen2.5-VL-3B-Instruct --judge gpt-4o --noise_name gaussian --ratio 0.9
```
**Command for evaluating an API model**
Example:
```
python run.py --data MMBench_Video_8frame_nopack --model GPT4V --judge gpt-4o 
```
The configuration remains identical to the above settings. Simply append specific noise parameters afterward, but ensure API calls are restricted to single-process execution.
To disable noise addition, simply omit the `noise` and `ratio` parameters. 

# **Step 3: Evaluation**
Following prior work, we incorporate a traditional metric in our benchmark: the ​**​GPT score​**​. However, as this metric relies solely on a single model’s judgment, we propose complementary evaluations: (1) the ​**​SBERT score​**​ for semantic alignment, and (2) ​**​accuracy​**​ on selection-based tasks as statistical indicators.

`evaluation.py` computes the ​​**​GPT score​**, **​SBERT score​**​ and ​**​Accuracy for True/False questions​**​ across multiple perspectives, supporting diverse Video-LLMs and noise parameters.
The input is sourced from prediction results generated by `run.py`, so please run `run.py` before executing `evaluation.py`.

**Argrments**

 - `--ratio (int)`: **Proportion** of noise frames.
 - `--model (list[str])`: **List** of model names to process.
 - `--noise (list[str])`: **List** of noise types to process.
 - `--metric (str, choice are ['acc', 'sbert', 'gpt'])`: Metric to compute.
 - `--perspective (str, choice are ['qtype', 'vtype'])`: Perspective to analyse.

Example:
```
python evaluation.py --metric gpt --noise gaussian --model Qwen2.5-VL-3B-Instruct --ratio 0.9 --perspective qtype
```
The evaluation results will be printed as logs, besides. **Result Files** will also be generated in the directory `$WORK_DIR/{model_name}/{noise_name}`(If no noise is applied, the filename defaults to `"origin"`) including  **GPT score**, **SBERT Score**, and **Accuracy for True/False questions** across **multiple perspectives**.
- **.xlsx files**: Contain inference results.
- **rating.json/gpt\*.json**: Stores the overall **GPT score** and per-question-type breakdown.
- **acc\*.json**: Stores the overall **Accuracy for True/False questions** and per-question-type breakdown.
- **sbert\*.json**: Stores the overall **SBERT Score** and per-question-type breakdown.
- **score\*.xlsx**: Records scores for each individual QA pair.


